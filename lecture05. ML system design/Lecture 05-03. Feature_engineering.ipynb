{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 05. Конструирование признаков (Feature Engineering) и отбор признаков (Feature Selection)\n",
    "В этом курсе, мы уже видели несколько ключевых алгоритмов машинного обучения. Однако, прежде чем перейти к более сложным алгоритмам, мы хотели бы сделать небольшое отступление и поговорить о подготовке данных. Известная концепция \"garbage in — garbage out(мусор на входе-мусор на выходе)\" применима на 100% к любой задаче в машинном обучении. Любой опытный специалист может вспомнить множество случаев, когда простая модель, обученная на высококачественных данных, оказывалась лучше, чем сложный ансамбль из нескольких моделей, построенный на данных, которые были недостаточно чистыми.\n",
    "\n",
    "Для начала я хотел рассмотреть три похожие, но разные задачи:\n",
    "* **feature extraction** и **feature engineering**: преобразование необработанных данных в объекты, пригодные для моделирования;\n",
    "* **feature transformation**: преобразование данных для повышения точности алгоритма;\n",
    "* **feature selection**: удаление ненужных признаков.\n",
    "\n",
    "Эта статья почти не будет содержать математику, но будет довольно много кода. В некоторых примерах будет использоваться набор данных от компании Renthop, который используется в [Two Sigma Connect: Rental Listing Inquiries Kaggle competition](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries). Файл `train.json` также хранится [здесь](https://drive.google.com/open?id=1_lqydkMrmyNAgG4vU4wVmp6-j7tV0XI8) как `renthop_train.json.gz` (так что, сначала распакуйте его). В этой задаче вам нужно спрогнозировать популярность объявлений об аренде, т. е. разделите листинг объявлений на три класса:`['low', 'medium' , 'high']`. Для оценки решений мы будем использовать показатель log loss (чем меньше, тем лучше). Те, у кого нет учетной записи Kaggle, должны будут зарегистрироваться; вам также нужно будет принять правила конкурса, чтобы загрузить данные. \n",
    "\n",
    "#автоматическая предварительная загрузка набора данных, если он еще не установлен.\n",
    "import os, requests\n",
    "\n",
    "url = 'https://drive.google.com/uc?export=download&id=1_lqydkMrmyNAgG4vU4wVmp6-j7tV0XI8'\n",
    "file_name = '../../data/renthop_train.json.gz'\n",
    "\n",
    "def load_renthop_dataset(url, target, overwrite=False):\n",
    "    #check if exists already\n",
    "    if os.path.isfile(target) and not overwrite:\n",
    "        print(\"Dataset is already in place\")\n",
    "        return\n",
    "    \n",
    "    print(\"Will download the dataset from\", url)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    open(target, 'wb').write(response.content)\n",
    "\n",
    "load_renthop_dataset(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:07.067528Z",
     "start_time": "2018-03-15T14:06:02.181930Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../../data/renthop_train.json.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краткое содержание\n",
    "\n",
    "1. Feature Extraction (Извлечение признаков)\n",
    "        1. Тексты\n",
    "        2. Изображения\n",
    "        3. Геоданные\n",
    "        4. Дата и время\n",
    "        5. Временные ряды, веб и т.д.\n",
    "\n",
    "2. Feature transformations (Преобразование признаков)\n",
    "        1. Нормализация и изменение распределения\n",
    "        2. Взаимодействия\n",
    "        3. Заполнение пропусков\n",
    "\n",
    "3. Feature selection (Выбор признаков)\n",
    "        1. Статистические подходы\n",
    "        2. Выбор путем построения модели\n",
    "        3. Перебор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "На практике данные редко поступают в виде готовых к использованию матриц. Вот почему каждая задача начинается с извлечения признаков. Иногда, может быть достаточно, прочитать файл csv и конвертировать его в `numpy.array`, но это редкое исключение. Давайте рассмотрим некоторые популярные типы данных, из которых можно извлечь признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тексты\n",
    "\n",
    "Текст-это тип данных, который может поступать в разных форматах; существует множество методов обработки текста, которые не могут поместиться в одной статье. Тем не менее, мы рассмотрим самые популярные из них.\n",
    "\n",
    "Прежде чем работать с текстом, нужно его токенизировать. Токенизация подразумевает разбиение текста на единицы (следовательно, токены). В самом простом варианте токены - это просто слова. Но разделяя по словам мы может потерять часть смысла - \"Санта Барбара\" - это один токен, а не два, но слово \"рок-н-ролл\" должен быть разделен на два токена. Существуют готовые к использованию токенизаторы, учитывающие особенности языка, но они также допускают ошибки, особенно при работе со специфическими текстами (газетами, сленгом, орфографическими ошибками, опечатками).\n",
    "\n",
    "После токенизации вы нормализуете данные. Для текста, речь идет о stemming'е и/или лемматизации; это похожие процессы, используемые для обработки различных форм слова. Можно прочитать о разнице между ними [здесь](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).\n",
    "\n",
    "Итак, теперь, когда мы превратили документ в последовательность слов, мы можем представить его в качестве векторов. Самый простой подход называется Bag of Words (мешок слов): мы создаем вектор с длиной словаря, вычисляем количество вхождений каждого слова в тексте и помещаем это количество вхождений в соответствующую позицию в векторе. Описанный процесс проще выглядит в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:07.087385Z",
     "start_time": "2018-03-15T14:06:07.068964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: [(0, 'dog'), (1, 'cat'), (2, 'and'), (3, 'you'), (4, 'i'), (5, 'have'), (6, 'a')]\n",
      "Vectors:\n",
      "[ 0.  1.  0.  0.  1.  1.  1.]\n",
      "[ 1.  0.  0.  1.  0.  1.  1.]\n",
      "[ 1.  1.  2.  1.  1.  1.  2.]\n"
     ]
    }
   ],
   "source": [
    "texts = ['i have a cat', \n",
    "         'you have a dog', \n",
    "         'you and i have a cat and a dog']\n",
    "\n",
    "vocabulary = list(enumerate(set([word for sentence \n",
    "                                 in texts for word in sentence.split()])))\n",
    "print('Vocabulary:', vocabulary)\n",
    "\n",
    "def vectorize(text): \n",
    "    vector = np.zeros(len(vocabulary)) \n",
    "    for i, word in vocabulary:\n",
    "        num = 0 \n",
    "        for w in text: \n",
    "            if w == word: \n",
    "                num += 1 \n",
    "        if num: \n",
    "            vector[i] = num \n",
    "    return vector\n",
    "\n",
    "print('Vectors:')\n",
    "for sentence in texts: \n",
    "    print(vectorize(sentence.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот иллюстрация этого процесса:\n",
    "\n",
    "<img src='../../img/bag_of_words.png' width=50%>\n",
    "\n",
    "Это крайне наивная реализация. На практике необходимо учитывать стоп-слова, максимальную длину словаря, более эффективные структуры данных (обычно текстовые данные преобразуются в разреженный вектор) и т.д.\n",
    "\n",
    "При использовании алгоритмов типа Bag of Words мы теряем порядок слов в тексте, что означает, что тексты \"у меня нет коров\" и \"нет, у меня есть коровы\" будут выглядеть идентичными после векторизации, когда на самом деле они имеют противоположное значение. Чтобы избежать этой проблемы, мы можем вернуться к нашему шагу токенизации и использовать вместо этого N-граммы (последовательность *из N последовательных токенов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:08.998673Z",
     "start_time": "2018-03-15T14:06:07.088376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.002804Z",
     "start_time": "2018-03-15T14:06:08.999801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': 2, 'have': 1, 'cows': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.110448Z",
     "start_time": "2018-03-15T14:06:09.003924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.218867Z",
     "start_time": "2018-03-15T14:06:09.113204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': 4,\n",
       " 'have': 1,\n",
       " 'cows': 0,\n",
       " 'no have': 6,\n",
       " 'have cows': 2,\n",
       " 'have no': 3,\n",
       " 'no cows': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:13:25.767656Z",
     "start_time": "2018-03-14T14:13:25.763924Z"
    }
   },
   "source": [
    "Также обратите внимание, что не нужно использовать только слова. В некоторых случаях можно сгенерировать N-граммы символов. Этот подход мог бы учитывать сходство связанных слов или обрабатывать опечатки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.774148Z",
     "start_time": "2018-03-15T14:06:09.220060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8284271247461903, 3.1622776601683795, 3.3166247903553998)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3, 3), analyzer='char_wb')\n",
    "\n",
    "n1, n2, n3, n4 = vect.fit_transform(['andersen', 'petersen', 'petrov', 'smith']).toarray()\n",
    "\n",
    "euclidean(n1, n2), euclidean(n2, n3), euclidean(n3, n4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Развитие идеи Bag of Words: слова, которые редко встречаются в корпусе (во всех рассматриваемых документах этого набора данных), но присутствуют в этом конкретном документе, могут оказаться более важными. Тогда имеет смысл повысить вес более узкотематическим словам, чтобы отделить их от общетематических. Этот подход называется TF-IDF (term frequency-inverse document frequency), его уже не напишешь в несколько строк кода, потому желающие могут ознакомиться с деталями во внешних источниках вроде [wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Вариант по умолчанию выглядит так:\n",
    "\n",
    "$$ \\large idf(t,D) = \\log\\frac{\\mid D\\mid}{df(d,t)+1} $$\n",
    "\n",
    "$$ \\large tfidf(t,d,D) = tf(t,d) \\times idf(t,D) $$\n",
    "\n",
    "Аналоги Bag of words могут встречаться и за пределами текстовых задач: например, bag of sites в соревнавании [Catch Me If You Can](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking), [bag of apps](https://www.kaggle.com/xiaoml/talkingdata-mobile-user-demographics/bag-of-app-id-python-2-27392), [bag of events](http://www.interdigital.com/download/58540a46e3b9659c9f000372), etc.\n",
    "\n",
    "![image](../../img/bag_of_words.png)\n",
    "\n",
    "Используя эти алгоритмы, можно получить рабочее решение для простой задачи, которое может служить в качестве базы (baseline). Однако для тех, кто не любит классику, существуют новые подходы. Самый популярный метод новой волны - [Word2Vec](https://arxiv.org/pdf/1310.4546.pdf), но есть и альтернативы ([GloVe](https://nlp.stanford.edu/pubs/glove.pdf), [Fasttext](https://arxiv.org/abs/1607.01759), etc.).\n",
    "\n",
    "Word2Vec является частным случаем алгоритмов векторного представления (word embedding). Используя Word2Vec и подобные модели, мы можем не только векторизовать слова в многомерном пространстве (обычно несколько сотен измерений), но и сравнить их семантическое сходство. Это классический пример операций, которые могут быть выполнены на векторизованных представлениях: король-мужчина + женщина = королева (king - man + woman = queen).\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/800/1*K5X4N-MJKt8FGFtrTHwidg.gif)\n",
    "\n",
    "Стоит отметить, что эта модель не обладает пониманием слов, а просто пытается расположить векторы таким образом, чтобы слова, используемые в общем контексте, были близки друг к другу. Если это не будет учтено, появится много забавных курьезов.\n",
    "\n",
    "Такие модели должны быть обучены на очень больших наборах данных, чтобы векторные координаты могли обхватить всю семантику. Предварительно подготовленную модель для ваших собственных задач можно загрузить [здесь](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models).\n",
    "\n",
    "Аналогичные методы применяются и в других областях, таких как биоинформатика. Неожиданным применением является [food2vec](https://jaan.io/food2vec-augmented-cooking-machine-intelligence/). Вы, вероятно, можете придумать несколько других свежих идей; концепция достаточно универсальна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изображения\n",
    "\n",
    "Работать с изображениями проще и сложнее одновременно. Это проще, потому что можно просто использовать одну из популярных предварительно обученных сетей без особых размышлений, но сложнее, потому что, если вам нужно копаться в деталях, вы можете в конечном итоге пойти очень глубоко. Давайте начнем с самого начала.\n",
    "\n",
    "В то время, когда графические процессоры были слабее и \"Ренессанс нейронных сетей\" еще не случился, генерация признаков из изображений была особенно сложной областью. Приходилось работать на низком уровне, определяя углы, границы областей, статистику распределения цветов и так далее. Опытные специалисты в области компьютерного зрения могли бы провести много параллелей между старыми подходами и нейронными сетями; в частности, сверточные слои в современных сетях похожи на [каскады Хаара](https://en.wikipedia.org/wiki/Haar-like_feature). Если вы захотите прочитать больше, вот несколько ссылок на некоторые интересные библиотеки: [skimage](http://scikit-image.org/docs/stable/api/skimage.feature.html) и [SimpleCV](http://simplecv.readthedocs.io/en/latest/SimpleCV.Features.html).\n",
    "\n",
    "Часто для решения задач, связанных с изображениями, используется сверточная нейронная сеть. Вам не придется придумывать архитектуру и обучать сеть с нуля. Вместо этого загрузите предварительно подготовленную state of the art сеть с весами из открытых источников. Специалисты по обработке данных часто делают так называемую тонкую настройку, чтобы адаптировать эти сети к своим потребностям, \"отсоединяя\" последние полностью подключенные слои сети и добавляя новые слои, выбранные для конкретной задачи, а затем обучая сеть на новых данных. Если ваша задача состоит в том, чтобы просто векторизовать изображение (например, использовать какой-то несетевой классификатор), вам нужно только удалить последние слои и использовать выходные данные из предыдущих слоев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:25.714680Z",
     "start_time": "2018-03-15T14:06:09.775547Z"
    }
   },
   "outputs": [],
   "source": [
    "# doesn't work with Python 3.7 \n",
    "# # Install Keras and tensorflow (https://keras.io/)\n",
    "# from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "# from keras.preprocessing import image \n",
    "# from scipy.misc import face \n",
    "# import numpy as np\n",
    "\n",
    "# resnet_settings = {'include_top': False, 'weights': 'imagenet'}\n",
    "# resnet = ResNet50(**resnet_settings)\n",
    "\n",
    "# # What a cute raccoon!\n",
    "# img = image.array_to_img(face())\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.770041Z",
     "start_time": "2018-03-15T14:06:25.718729Z"
    }
   },
   "outputs": [],
   "source": [
    "# # In real life, you may need to pay more attention to resizing\n",
    "# img = img.resize((224, 224))\n",
    "\n",
    "# x = image.img_to_array(img) \n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# x = preprocess_input(x)\n",
    "\n",
    "# # Need an extra dimension because model is designed to work with an array\n",
    "# # of images - i.e. tensor shaped (batch_size, width, height, n_channels)\n",
    "\n",
    "# features = resnet.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:44:25.102755Z",
     "start_time": "2018-03-14T14:44:24.374869Z"
    }
   },
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/800/1*Iw_cKFwLkTVO2SPrOZU2rQ.png' width=60%>\n",
    "\n",
    "*Вот классификатор, обученный на одном наборе данных и адаптированный для другого, \"отсоединием\" последнего слоя и добавлением вместо него нового.*\n",
    "\n",
    "Тем не менее, мы не должны слишком сосредотачиваться на нейросетевых методах. Функции, генерируемые вручную, по-прежнему очень полезны: например, для прогнозирования популярности объявления аренды можно предположить, что светлые квартиры привлекают больше внимания и создают такой признак, как \"среднее значение пикселя\". Вы можете найти некоторые вдохновляющие примеры в документации [соответствующих библиотек](http://pillow.readthedocs.io/en/3.1.x/reference/ImageStat.html).\n",
    "\n",
    "Если на картинке ожидается текст, его также можно прочитать и не разворачивая своими руками сложную нейросеть: например, при помощи [pytesseract](https://github.com/madmaze/pytesseract)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:47:46.671934Z",
     "start_time": "2018-03-14T14:47:43.945326Z"
    }
   },
   "source": [
    "```python\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "##### Just a random picture from search\n",
    "img = 'http://ohscurrent.org/wp-content/uploads/2015/09/domus-01-google.jpg'\n",
    "\n",
    "img = requests.get(img)\n",
    "img = Image.open(BytesIO(img.content))\n",
    "text = pytesseract.image_to_string(img)\n",
    "\n",
    "text\n",
    "\n",
    "Out: 'Google'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно понимать, что `pytesseract` - это решение не для всех случаев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "##### This time we take a picture from Renthop\n",
    "img = requests.get('https://photos.renthop.com/2/8393298_6acaf11f030217d05f3a5604b9a2f70f.jpg')\n",
    "img = Image.open(BytesIO(img.content))\n",
    "pytesseract.image_to_string(img)\n",
    "\n",
    "Out: 'Cunveztible to 4}»'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой случай, когда нейронные сети не могут помочь, - это извлечение признаков из метаинформации. У изображений файл EXIF хранит много полезной метаинформации: производитель и модель камеры, разрешение, использование вспышки, географические координаты съемки, программное обеспечение, используемое для обработки изображения и многое другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Геоданные\n",
    "\n",
    "Географические данные не так часто встречаются в задачах, но все же полезно овладеть основными приемами работы с ними, тем более что в этой области существует довольно много готовых решений.\n",
    "\n",
    "Геопространственные данные часто представляются в виде адресов или координат (широта, долгота). В зависимости от поставленной задачи могут потребоваться две взаимно-обратные операции: геокодирование (восстановление точки из адреса) и обратное геокодирование (восстановление адреса из точки). Обе операции доступны на практике через внешние API из Google Maps или OpenStreetMap. Различные геокодеры имеют свои особенности, и качество варьируется от региона к региону. К счастью, существуют универсальные библиотеки, такие как [geopy](https://github.com/geopy/geopy), которые действуют как оболочки для этих внешних служб.\n",
    "\n",
    "Если у вас много данных, вы быстро достигнете пределов внешнего API. Кроме того, это не всегда самый быстрый способ получить информацию через HTTP. Следовательно, необходимо рассмотреть возможность использования локальной версии проекта OpenStreetMap.\n",
    "\n",
    "Если у вас есть небольшой объем данных, достаточно времени, и нет желания извлекать причудливые признаки, вы можете использовать `reverse_geocoder` вместо OpenStreetMap:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T15:12:50.468269Z",
     "start_time": "2018-03-14T15:12:50.455393Z"
    }
   },
   "source": [
    "```python\n",
    "import reverse_geocoder as revgc\n",
    "\n",
    "revgc.search((df.latitude, df.longitude))\n",
    "Loading formatted geocoded file... \n",
    "\n",
    "Out: [OrderedDict([('lat', '40.74482'), \n",
    "                   ('lon', '-73.94875'), \n",
    "                   ('name', 'Long Island City'), \n",
    "                   ('admin1', 'New York'), \n",
    "                   ('admin2', 'Queens County'), \n",
    "                   ('cc', 'US')])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с геокодированием не следует забывать, что адреса могут содержать опечатки, что говорит о необходимости очистки данных. Координаты содержат меньше опечаток, но их положение может быть неправильным из-за шума GPS или плохой точности в таких местах, как туннели, районы в центре города и т.д. Если источником данных является мобильное устройство, геолокация может определяться не по GPS, а сетями WiFi в этом районе, что приводит к дырам в пространстве и телепортации. Во время путешествия по Манхэттену, вдруг может быть точка Wi-Fi из Чикаго.\n",
    "\n",
    "> Отслеживание местоположения WiFi основано на комбинации SSID и MAC-адресов, которые могут соответствовать различным точкам, например, федеральный провайдер стандартизирует прошивку маршрутизаторов до MAC-адреса и размещает их в разных городах. Даже переезд компании в другой офис с ее маршрутизаторами может вызвать проблемы.\n",
    "\n",
    "Точка, как правило, находится среди инфраструктуры. Здесь вы действительно можете раскрыть свое воображение и придумать параметры, основанные на вашем жизненном опыте и знании предметной области: близость точки к метро, количество этажей в здании, расстояние до ближайшего магазина, количество банкоматов вокруг и т.д. Для любой задачи вы можете легко придумать десятки параметров и извлечь их из различных внешних источников. Для проблем вне городской среды, вы можете рассмотреть признаки из более конкретных источников, например, высота над уровнем моря.\n",
    "\n",
    "Если две или более точек связаны между собой, возможно, стоит извлечь объекты из маршрута между ними. В этом случае будут полезны расстояния (стоит смотреть и на great circle distance, и на \"честное\" расстояние, посчитанное по дорожному графу), количество поворотов с соотношением левых и правых поворотов, количество светофоров, перекрестков и мостов. В одной из моих собственных задач я создал функцию под названием \"сложность дороги\", которая вычисляла графически рассчитанное расстояние, разделенное на GCD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дата и время\n",
    "\n",
    "Можно подумать, что дата и время стандартизированы из-за их распространенности, но тем не менее, некоторые подводные камни остаются.\n",
    "\n",
    "Начнем с дня недели, который легко превратить в 7 фиктивных(dummy) переменных, используя однократное(one-hot) кодирование. Кроме того, мы также создадим отдельную двоичную функцию для выходных под названием `is_weekend`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df['dow'] = df['created'].apply(lambda x: x.date().weekday())\n",
    "df['is_weekend'] = df['created'].apply(lambda x: 1 if x.date().weekday() in (5, 6) else 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для некоторых задач могут потребоваться дополнительные функции календаря. Например, снятие наличных может быть привязано к платежному дню; покупка карты метро - к началу месяца. В целом, при работе с данными временных рядов неплохо иметь календарь с праздничными днями, ненормальными погодными условиями и другими важными событиями.\n",
    "\n",
    "> Вопрос: Что общего у китайского Нового года, Нью-Йоркского марафона и инаугурации Трампа?\n",
    "\n",
    "> Ответ: все они должны быть занесены в календарь потенциальных аномалий.\n",
    "\n",
    "Работать с часом (минутой, днем месяца ...) не так просто, как кажется. Если вы используете час как реальную переменную, мы немного противоречим природе данных: `0<23`, в то время как`0:00:00 02.01> 01.01 23:00:00`. Для некоторых проблем это может иметь решающее значение. В то же время, если вы закодируете их как категориальные переменные, вы породите большое количество признаков и потеряете информацию о близости-разница между 22 и 23 будет такой же, как разница между 22 и 7.\n",
    "\n",
    "Существуют также некоторые более эзотерические подходы к таким данным, как проецирование времени на круг и использование двух координат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.782320Z",
     "start_time": "2018-03-15T14:06:27.772449Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_harmonic_features(value, period=24):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value), np.sin(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это преобразование сохраняет расстояние между точками, что важно для алгоритмов, оценивающих расстояние (kNN, SVM, k-means ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.883311Z",
     "start_time": "2018-03-15T14:06:27.784833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5176380902050424"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "euclidean(make_harmonic_features(23), make_harmonic_features(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:28.250852Z",
     "start_time": "2018-03-15T14:06:27.884753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5176380902050414"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(make_harmonic_features(9), make_harmonic_features(11)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:28.801865Z",
     "start_time": "2018-03-15T14:06:28.252109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(make_harmonic_features(9), make_harmonic_features(21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако разница между такими методами кодирования сводится к третьему десятичному знаку в метрике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Временные ряды, web, и т.д.\n",
    "\n",
    "Что касается временных рядов — мы не будем вдаваться в детали здесь (в основном из - за моего личного отсутствия опыта), но я укажу вам на [полезную библиотеку, которая автоматически генерирует функции для временных рядов](https://github.com/blue-yonder/tsfresh).\n",
    "\n",
    "Если вы работаете с веб-данными, то обычно у вас есть информация о User Agent пользователя. Это огромное количество информации. Во-первых, нужно извлечь из него операционную систему. Во-вторых, сделайть пизнак `is_mobile`. В-третьих, посмотреть на браузер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.336832Z",
     "start_time": "2018-03-15T14:06:28.804134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a bot?  False\n",
      "Is mobile?  False\n",
      "Is PC?  True\n",
      "OS Family:  Ubuntu\n",
      "OS Version:  ()\n",
      "Browser Family:  Chromium\n",
      "Browser Version:  (56, 0, 2924)\n"
     ]
    }
   ],
   "source": [
    "# Install pyyaml ua-parser user-agents\n",
    "import user_agents\n",
    "\n",
    "ua = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/56.0.2924.76 Chrome/56.0.2924.76 Safari/537.36'\n",
    "ua = user_agents.parse(ua)\n",
    "\n",
    "print('Is a bot? ', ua.is_bot)\n",
    "print('Is mobile? ', ua.is_mobile)\n",
    "print('Is PC? ',ua.is_pc)\n",
    "print('OS Family: ',ua.os.family)\n",
    "print('OS Version: ',ua.os.version)\n",
    "print('Browser Family: ',ua.browser.family)\n",
    "print('Browser Version: ',ua.browser.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:11:22.538721Z",
     "start_time": "2018-03-14T16:11:22.534198Z"
    }
   },
   "source": [
    "> Как и в других областях, вы можете придумать свои собственные признаки, основанные на интуиции о природе данных. На момент написания этой статьи Chromium 56 был новым, но через некоторое время только пользователи, которые не перезагружали свой браузер в течение длительного времени, будут иметь эту версию. В этом случае, почему бы не ввести функцию под названием \"отставание от последней версии браузера\"?\n",
    "\n",
    "В дополнение к операционной системе и браузеру, вы можете посмотреть на реферер (не всегда доступный), [http_accept_language](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language), и другую метаинформацию.\n",
    "\n",
    "Следующая полезная информация-это IP-адрес, из которого можно извлечь страну и, возможно, город, провайдера и тип подключения (мобильный/стационарный). Вы должны понимать, что существует множество прокси и устаревших баз данных, поэтому эта функция может содержать шум. Гуру сетевого администрирования могут попытаться извлечь даже более сложные пизнаки, такие как предложения по [использованию VPN](https://habrahabr.ru/post/216295/). Кстати, данные с IP-адреса хорошо сочетаются с `http_accept_language`: если пользователь сидит на чилийских прокси, а локаль браузера `ru_RU`, то что-то нечисто и стоит посмотреть в соответствующем столбце таблицы (`is_traveler_or_proxy_user`).\n",
    "\n",
    "Любая данная область имеет так много особенностей, что это слишком много для человека, чтобы узнать о них полностью. Поэтому я приглашаю всех поделиться своим опытом и обсудить извлечение и генерацию признаков в разделе комментариев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование объектов\n",
    "\n",
    "### Нормализация и изменение распределения\n",
    "\n",
    "Монотонное преобразование признаков имеет решающее значение для некоторых алгоритмов и не влияет на другие. Это одна из причин возросшей популярности деревьев решений и всех их производных алгоритмов (случайный лес, градиентный бустинг). Не все могут или хотят возиться с преобразованиями, а эти алгоритмы устойчивы к необычным распределениям.\n",
    "\n",
    "Есть и чисто инженерные причины: `np.log` - это способ работы с большими числами, которые не вписываются в `np.float64`. Это скорее исключение, чем правило; часто оно вызвано желанием адаптировать набор данных к требованиям алгоритма. Параметрические методы обычно требуют минимального симметричного и унимодального распределения данных, которое не всегда дается в реальных данных. Там могут быть более жесткие требования; напомним [предыдущую лекцию о линейных моделях](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220).\n",
    "\n",
    "Однако требования к данным предъявляют не только параметрические методы; [K nearest neighbors](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd) будет предсказывать полную бессмыслицу, если объекты не нормализованы, например: когда одно распределение находится в окрестности нуля и не выходит за пределы (-1, 1), в то время как диапазон другого порядка сотен тысяч.\n",
    "\n",
    "Простой пример: предположим, что задача состоит в том, чтобы спрогнозировать стоимость квартиры по двум переменным — удаленности от центра города и количеству комнат. Количество комнат редко превышает 5, тогда как расстояние от центра города легко может измеряться тысячами метров.\n",
    "\n",
    "Простейшее преобразование - это стандартное масштабирование (Standard Scaling) (или Z-score normalization):\n",
    "\n",
    "$$ \\large z= \\frac{x-\\mu}{\\sigma} $$\n",
    "\n",
    "Обратите внимание, что Standard Scaling не делает распределение нормальным в строгом смысле этого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.382748Z",
     "start_time": "2018-03-15T14:06:29.338320Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "\n",
    "data = beta(1, 10).rvs(1000).reshape(-1, 1)\n",
    "shapiro(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.509590Z",
     "start_time": "2018-03-15T14:06:29.385020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Value of the statistic, p-value\n",
    "shapiro(StandardScaler().fit_transform(data))\n",
    "\n",
    "# With such p-value we'd have to reject the null hypothesis of normality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но, в какой-то степени, защищает от выбросов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.602528Z",
     "start_time": "2018-03-15T14:06:29.511150Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([1, 1, 0, -1, 2, 1, 2, 3, -2, 4, 100]).reshape(-1, 1).astype(np.float64)\n",
    "StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.713603Z",
     "start_time": "2018-03-15T14:06:29.605288Z"
    }
   },
   "outputs": [],
   "source": [
    "(data - data.mean()) / data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним довольно популярным вариантом является MinMax Scaling, который переносит все точки в заданный интервал (обычно (0, 1)).\n",
    "\n",
    "$$ \\large X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.855619Z",
     "start_time": "2018-03-15T14:06:29.716000Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "MinMaxScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.955155Z",
     "start_time": "2018-03-15T14:06:29.857042Z"
    }
   },
   "outputs": [],
   "source": [
    "(data - data.min()) / (data.max() - data.min()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaling и MinMax Scaling имеют схожие области применения и часто более или менее взаимозаменяемы. Однако если алгоритм предполагает вычисление расстояний между точками или векторами, то по умолчанию используется StandardScaling. Нри этом Min Max Scaling полезен для визуализации, чтобы перенести признаки на отрезок (0, 255).\n",
    "\n",
    "Если мы предположим, что некоторые данные не распределены нормально, но описываются [логонормальным распределением](https://en.wikipedia.org/wiki/Log-normal_distribution), его можно легко преобразовать к нормальному распределению:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:30.067680Z",
     "start_time": "2018-03-15T14:06:29.957011Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "\n",
    "data = lognorm(s=1).rvs(1000)\n",
    "shapiro(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:30.180348Z",
     "start_time": "2018-03-15T14:06:30.069180Z"
    }
   },
   "outputs": [],
   "source": [
    "shapiro(np.log(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логнормальное распределение подходит для описания зарплат, стоимости ценных бумаг, городского населения, количества комментариев к статьям в интернете и т.д. Однако для применения этой процедуры базовое распределение необязательно должно быть логнормальным; вы можете попробовать применить это преобразование к любому распределению с тяжелым правым хвостом. Кроме того, можно попытаться использовать и другие подобные преобразования, формулируя собственные гипотезы о том, как приблизить имеющееся распределение к нормали. Примерами таких преобразований являются [преобразование Бокса-Кокса](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) (логарифм-это частный случай преобразования Бокса-Кокса) или [преобразование Yeo-Johnson](https://gist.github.com/mesgarpour/f24769cd186e2db853957b10ff6b7a95) (расширяет диапазон применяемости на отрицательные числа). Кроме того, вы также можете попробовать добавить константу к признаку — `np.log (x + const)`.\n",
    "\n",
    "В приведенных выше примерах мы работали с синтетическими данными и строго проверяли нормальность с помощью теста Шапиро-Уилка. Давайте попробуем взглянуть на некоторые реальные данные и проверить нормальность с помощью менее формального метода - [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot). Для нормального распределения она будет выглядеть как гладкая диагональная линия, а визуальные аномалии должны быть интуитивно понятны.\n",
    "\n",
    "\n",
    "![image](../../img/qq_lognorm.png)\n",
    "Q-Q график для логнормального распределения\n",
    "\n",
    "![image](../../img/qq_log.png)\n",
    "Q-Q график для того же распределения после логарифмирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:31.801140Z",
     "start_time": "2018-03-15T14:06:30.182573Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's draw plots!\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Let's take the price feature from Renthop dataset and filter by hands the most extreme values for clarity\n",
    "\n",
    "price = df.price[(df.price <= 20000) & (df.price > 500)]\n",
    "price_log = np.log(price)\n",
    "\n",
    "# A lot of gestures so that sklearn didn't shower us with warnings\n",
    "price_mm = MinMaxScaler().fit_transform(price.values.reshape(-1, 1).astype(np.float64)).flatten()\n",
    "price_z = StandardScaler().fit_transform(price.values.reshape(-1, 1).astype(np.float64)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Q график исходного признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:32.717805Z",
     "start_time": "2018-03-15T14:06:31.802677Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price, loc=price.mean(), scale=price.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Q график признака после StandartScaler. Форма не меняется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:32.987753Z",
     "start_time": "2018-03-15T14:06:32.719093Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price_z, loc=price_z.mean(), scale=price_z.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Q график признака после MinMaxScaler. Форма не меняется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:33.243510Z",
     "start_time": "2018-03-15T14:06:32.988869Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price_mm, loc=price_mm.mean(), scale=price_mm.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Q график признака после логарифмирования. Все стало значительно лучше!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:33.510998Z",
     "start_time": "2018-03-15T14:06:33.244652Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price_log, loc=price_log.mean(), scale=price_log.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, могут ли преобразования как-то помочь реальной модели. Здесь нет серебряной пули (Нет универсального метода. Фредерик Брукс(с))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions (Взаимодействия)\n",
    "\n",
    "Если предыдущие преобразования казались скорее математическими, то эта часть больше касается природы данных; ее можно отнести как к преобразованиям объектов, так и к созданию объектов.\n",
    "\n",
    "Давайте еще раз вернемся к задаче Two Sigma Connect: Rental Listing Inquiries. Среди признаков в этой задачи есть количество комнат и цена. Логика подсказывает, что стоимость одной комнаты более показательна, чем общая стоимость, поэтому мы можем выделить такуй признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:33.800002Z",
     "start_time": "2018-03-15T14:06:33.512381Z"
    }
   },
   "outputs": [],
   "source": [
    "rooms = df[\"bedrooms\"].apply(lambda x: max(x, .5))\n",
    "# Avoid division by zero; .5 is chosen more or less arbitrarily\n",
    "df[\"price_per_bedroom\"] = df[\"price\"] / rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы должны ограничить себя в этом процессе. Если существует ограниченное число признаков, можно создать все возможные их взаимодействия, а затем отсеять ненужные, используя методы, описанные в следующем разделе. Кроме того, не все взаимодействия между признаками должны иметь физический смысл; например, полиномиальные объекты (см. [sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)) часто используются в линейных моделях и практически не поддаются интерпретации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in the missing values (Заполнение недостающих значений/пропусков)\n",
    "\n",
    "Не многие алгоритмы могут работать с пропущенными значениями, а реальный мир часто предоставляет данные с пробелами. К счастью, это одна из задач, для решения которой не требуется никакого творчества. Обе ключевые библиотеки python для анализа данных предоставляют простые в использовании решения: [pandas.DataFrame.fillna](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) и [sklearn.preprocessing.Imputer](http://scikit-learn.org/stable/modules/preprocessing.html#imputation).\n",
    "\n",
    "Эти решения не имеют никакого волшебства, происходящего за кулисами. Подходы к обработке пропущенных значений довольно просты:\n",
    "\n",
    "* одирование пропущенных значений отдельным пустым значением типа `n/a` (для категориальных переменных);\n",
    "* используйте наиболее вероятное значение признака (среднее или медиану для числовых переменных, наиболее распространенное значение для категориальных переменных);\n",
    "* или, наоборот, кодировать с некоторым экстремальным значением (хорошо для моделей дерева принятия решений, так как это позволяет модели сделать разделение между отсутствующими и не отсутствующими значениями);\n",
    "* для упорядоченных данных (например, временных рядов) возьмите соседнее значение — следующее или предыдущее.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/800/0*Ps-v8F0fBgmnG36S.)\n",
    "\n",
    "Простые в использовании библиотечные решения иногда предлагают придерживаться чего-то вроде `df = df.fillna(0)` и не потеть промежутки. Но это не самое лучшее решение: подготовка данных занимает больше времени, чем построение моделей, поэтому бездумное заполнение пробелов может скрыть ошибку в обработке и испортить модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (Выбор признаков)\n",
    "\n",
    "Зачем вообще нужно было выбирать признаки? Кому-то эта идея может показаться нелогичным, но есть по крайней мере две важные причины избавиться от несущественных признаков. Первое понятно каждому инженеру: чем больше данных, тем выше вычислительная сложность. Пока мы работаем с игрушечными наборами данных, размер данных не является проблемой, но для реальных загруженных производственных систем сотни дополнительных признаков будут вполне ощутимы. Вторая причина заключается в том, что некоторые алгоритмы принимают шум (неинформативный признак) в качестве сигнала и переобучаются.\n",
    "\n",
    "### Статистические подходы\n",
    "\n",
    "Наиболее очевидным кандидатом на удаление является признак, значение которого остается неизменным, т. е. он вообще не содержит никакой информации. Если мы будем опираться на эту мысль, то вполне разумно будет сказать, что признаки с низкой дисперсией хуже, чем признаки с высокой дисперсией. Таким образом, можно рассматривать отсечание признаков с дисперсией ниже определенного порога."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:34.669290Z",
     "start_time": "2018-03-15T14:06:33.801760Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "x_data_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:34.674947Z",
     "start_time": "2018-03-15T14:06:34.670899Z"
    }
   },
   "outputs": [],
   "source": [
    "VarianceThreshold(.7).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:34.801683Z",
     "start_time": "2018-03-15T14:06:34.676130Z"
    }
   },
   "outputs": [],
   "source": [
    "VarianceThreshold(.8).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:34.903665Z",
     "start_time": "2018-03-15T14:06:34.803278Z"
    }
   },
   "outputs": [],
   "source": [
    "VarianceThreshold(.9).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и другие способы, которые также [основаны на классической статистике](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.235212Z",
     "start_time": "2018-03-15T14:06:34.908459Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "x_data_kbest = SelectKBest(f_classif, k=5).fit_transform(x_data_generated, y_data_generated)\n",
    "x_data_varth = VarianceThreshold(.9).fit_transform(x_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='lbfgs', random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.251270Z",
     "start_time": "2018-03-15T14:06:35.237608Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(logit, x_data_generated, y_data_generated, \n",
    "                scoring='neg_log_loss', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.355729Z",
     "start_time": "2018-03-15T14:06:35.252493Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(logit, x_data_kbest, y_data_generated, \n",
    "                scoring='neg_log_loss', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.500340Z",
     "start_time": "2018-03-15T14:06:35.356862Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(logit, x_data_varth, y_data_generated, \n",
    "                scoring='neg_log_loss', cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что выбранные нами признаки улучшили качество классификатора. Конечно, этот пример является чисто искусственным, однако этот прием стоит использовать для реальных задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбор с использованием моделей\n",
    "\n",
    "Другой подход заключается в использовании некоторой базовой модели для оценки признаков, поскольку модель четко показывает их важность. Обычно используются два типа моделей: некоторые \"деревянные\" композиции, такие как [Random Forest](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7) или линейная модель с лассо(Lasso) регуляризацией , так что она склонна сводить к нулю веса слабых признаков. Логика интуитивно понятна: если функции явно бесполезны в простой модели, нет необходимости перетаскивать их в более сложную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.975262Z",
     "start_time": "2018-03-15T14:06:35.502079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic example\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=17)\n",
    "pipe = make_pipeline(SelectFromModel(estimator=rf), logit)\n",
    "\n",
    "print(cross_val_score(logit, x_data_generated, y_data_generated, \n",
    "                      scoring='neg_log_loss', cv=5).mean())\n",
    "print(cross_val_score(rf, x_data_generated, y_data_generated, \n",
    "                      scoring='neg_log_loss', cv=5).mean())\n",
    "print(cross_val_score(pipe, x_data_generated, y_data_generated, \n",
    "                      scoring='neg_log_loss', cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы не должны забывать, что это не универсальный метод - поэтому это может сделать производительность хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:36.095657Z",
     "start_time": "2018-03-15T14:06:35.976948Z"
    }
   },
   "outputs": [],
   "source": [
    "#x_data, y_data = get_data() \n",
    "x_data = x_data_generated\n",
    "y_data = y_data_generated\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), \n",
    "                      SelectFromModel(estimator=rf), logit)\n",
    "\n",
    "pipe2 = make_pipeline(StandardScaler(), logit)\n",
    "\n",
    "print('LR + selection: ', cross_val_score(pipe1, x_data, y_data, \n",
    "                                          scoring='neg_log_loss', cv=5).mean())\n",
    "print('LR: ', cross_val_score(pipe2, x_data, y_data, \n",
    "                              scoring='neg_log_loss', cv=5).mean())\n",
    "print('RF: ', cross_val_score(rf, x_data, y_data, \n",
    "                              scoring='neg_log_loss', cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перебор\n",
    "Наконец, мы переходим к самому надежному методу, который также является наиболее сложным с точки зрения вычислений: тривиальный поиск по сетке (передор). Обучите модель на подмножестве признаков, сохраните результаты, повторите для разных подмножеств и сравните качество моделей, чтобы определить лучший набор признаков. Этот подход называется [исчерпывающий выбор признаков](http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/).\n",
    "\n",
    "Поиск всех комбинаций обычно занимает слишком много времени, поэтому вы можете попытаться уменьшить пространство поиска. Зафиксируйте небольшое число N, перебирите все комбинации по N признаков, выберите лучшую комбинацию, а затем повторите комбинации из (N + 1) признаков так, чтобы предыдущая лучшая комбинация признаков была зафиксирована и рассматривался только один новый признак. Можно повторять до тех пор, пока мы не достигнем максимального количества характеристик или пока качество модели не перестанет значительно увеличиваться. Этот алгоритм называется [последовательный выбор признаков](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/).\n",
    "\n",
    "Этот алгоритм можно перевернуть: начать с полного пространства объектов и удалять объекты один за другим, пока это не ухудшит качество модели или пока не будет достигнуто желаемое количество объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:44.047841Z",
     "start_time": "2018-03-15T14:06:36.096849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "selector = SequentialFeatureSelector(logit, scoring='neg_log_loss', \n",
    "                                     verbose=2, k_features=3, forward=False, n_jobs=-1)\n",
    "\n",
    "selector.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите, как этот подход был реализован в одном [простом, но элегантном ядре Kaggle](https://www.kaggle.com/arsenyinfo/easy-feature-selection-pipeline-0-55-at-lb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Arseny Kravchenko](http://arseny.info/pages/about-me.html). Translated and edited by [Christina Butsko](https://www.linkedin.com/in/christinabutsko/), [Yury Kashnitskiy](https://yorko.github.io/), [Egor Polusmak](https://www.linkedin.com/in/egor-polusmak/), [Anastasia Manokhina](https://www.linkedin.com/in/anastasiamanokhina/), [Anna Larionova](https://www.linkedin.com/in/anna-larionova-74434689/), [Evgeny Sushko](https://www.linkedin.com/in/evgenysushko/) and [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
